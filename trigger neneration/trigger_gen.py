import os
import torch
import math
import numpy as np
from torch.autograd import Variable

def set_f_target(model, input):     #æ‰¾å‡ºè¾“å…¥æœ€å¤§çš„neuronå¹¶å°†å…¶target valueè®¾ç½®ä¸?0å€?
    output = model(input)
    f_neuron = output[1].cpu() #.numpy()
    m, n = f_neuron.shape
    index = int(f_neuron.argmax())
    x = int(index / n)
    y = index % n
    f_neuron[x,y] = 10 * f_neuron[x,y]
    return f_neuron

def loss_func(f_target, f_neuron, input):
    loss_fn = torch.nn.MSELoss(reduce=False, size_average=False)    #å‡æ–¹å·®loss
    loss = loss_fn(f_neuron, f_target)
    #loss.backward(torch.ones_like(loss))
    loss.sum().backward(retain_graph=True)     #è®¡ç®—losså¯¹inputçš„æ¢¯åº¦ï¼Œåœ¨inputè¾“å…¥ä¹‹å‰åº”å°†input.requires_gradè®¾ç½®ä¸ºTrue
    return input.grad, loss

def train_trigger(model, mask, trigger, epoch_max=200, cost_threshold=1, lr=.001):
    f_target = set_f_target(model, trigger)
    for i in range(epoch_max):
        output = model(trigger)
        f_neuron = output[1].cpu() #.numpy()
        x_, cost = loss_func(f_target, f_neuron, trigger)
        trigger = trigger - lr * (x_.mul(mask))
        i = i + 1
        if cost < cost_threshold:
            break
    return trigger

